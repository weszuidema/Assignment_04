---
title: "Assignment 04"
output:
  html_document:
    keep_md: true
    self_contained: false
    css: styles.css
    number_sections: true
    includes:
      in_header: includes/in_header.html
      before_body: includes/before_body.html
---

Instructions

1. [Fork this repository](https://help.github.com/articles/using-pull-requests/) to your GitHub account.
2. Write your solutions in R Markdown in a file named `index.Rmd`.
3. Compile your solutions to an HTML file, `index.html`. You can view it at `https://{username}.github.io/Assignment_04`.
3. When you are ready to submit your assignment, [initiate a pull request](https://help.github.com/articles/using-pull-requests/#initiating-the-pull-request). Title your
pull request "Submission".

To update your fork from the upstream repository:

1. On your fork, e.g. `https://github.com/jrnold/Assignment_04` click on "New Pull request"
2. Set your fork `jrnold/Assignment_04` as the base fork on the left, and `UW-POLS503/Assignment_04` as the head fork on the right. In both cases the branch will be master. This means, compare any canes in the head fork that are not in the base fork. You will see differences between the `US-POLS503` repository and your fork. Click on "Create Pull Request", and if there are no issues, "Click Merge" A quick way is to use this link, but change the `jrnold` to your own username: `https://github.com/jrnold/Assignment_04/compare/gh-pages...UW-POLS504:gh-pages`.

We'll use these packages,
```{r message=FALSE}
library("MASS")
library("pols503")
library("foreign")
library("dplyr")
library("broom")
library("ggplot2")
```
Since we are going to do some simulation, we should set a seed, so the results are exactly replicable.
```{r}
set.seed(1234)
```
Since some of these computations will take time, we can cache the results so that knitr will
only run code that has changed.
```{r}
knitr::opts_chunk$set(cache = TRUE, autodep = TRUE)
```

These are some utility functions used in this assignment
```{r}
mvnormal_data <- function(n,
                          mu = 0,
                          sigma = rep(1, length(mu)),
                          cor = diag(length(mu)),
                          empirical = TRUE,
                          colnames = paste0("x", seq_along(mu))
                          ) {
  setNames(as.data.frame(MASS::mvrnorm(n, mu = mu,
                                       sdcor2cov(sigma, cov2cor(cor)),
                              empirical = empirical)),
           colnames)
}
```
Given $\hat{\vec{y}}$ and $R^2$ calculate a regression standard deviation,
```{r}
r2_to_sigma <- function (yhat, r2) {
  ybar <- mean(yhat)
  ssm <- sum((yhat - ybar)^2)
  sse <- (1 - r2)/r2 * ssm
  sqrt(sse/ length(yhat))
}
```

Summarize the results of the simulations:
```{r}
summarize_params <- function(.data) {
  ret <- .data %>%
    group_by(term) %>%
    summarize(estimate_mean = mean(estimate),
              estimate_sd = sd(estimate),
              std_error_mean = mean(std.error),
              std_error_sd = sd(std.error),
              estimate_mean_se = sd(estimate) / sqrt(n()),
              estimate_sd_se = sd(estimate) / sqrt(2 * (n() - 1)),
              std_error_mean_sd = sd(std.error) / sqrt(n()),
              std_error_sd_se = sd(std.error) / sqrt(2 * (n() - 1)),
              iter = length(estimate))
  ret
}
```


# Monte-Carlo Simulations

## Multi-collinearity

```{r}
simulate_multicollinearity <- function(iter, n, rho) {
  beta <- c(0, 1, 1)
  r2 <- 0.5  
  cormat <- matrix(c(1, rho, rho, 1), nrow = 2, ncol = 2)
  dat <- mvnormal_data(n, mu = c(0, 0), cor = cormat)
  yhat <- model.matrix(~ x1 + x2, data = dat) %*% beta
  sigma <- r2_to_sigma(yhat, r2)
  results <- vector(mode = "list", length = iter)
  for (i in seq_len(iter)) {
    # Simulate y
    dat[["y"]] <- yhat + rnorm(n, sd = sigma)
    # Estimate OLS
    results <- tidy(lm(y ~ x1 + x2, data = dat))
  }
  summarize_params(results)
}

sims_multicollinearity <- expand.grid(rho = c(0, 0.3, 0.9),
            obs = c(30, 100, 1000)) %>%
  group_by(rho, obs) %>%
  do({
    simulate_multicollinearity(1500, rho = .$rho, n = .$obs)
  })
```

Create plots of the simulation results to answer the following questions:

- How is the bias of the coefficients related to the sample size ($n$) and correlation between $X_1$ and $X_2$ ($rho$)?

```{r}

```

- How is the size of the sampling distribution of $\hat{beta}_1$ related to ... ?

```{r}

```

- How is the bias of the standard error of $\hat{\beta}_1$ related to ...?

```{r}

```


## Measurement Error

```{r}
simulate_measurement_error <- function(iter, n, rho, reliability) {
  # Regression coefficients
  beta <- c(0, 1, 1)
  # Correlation between X
  cormat <- matrix(c(1, rho, rho, 1), nrow = 2, ncol = 2)
  # desired population R^2 (used to generate sigma)
  r2 <- 0.5
  # X drawn from a multivariate normal distribution
  dat <- mvnormal_data(n, mu = c(0, 0), cor = cormat)
  # yhat = X b
  yhat <- model.matrix(~ x1 + x2, data = dat) %*% beta
  # Regression standard deviation
  sigma <- r2_to_sigma(yhat, r2)
  results <- vector(mode = "list", length = iter)
  meas_sd <- (1 - reliability) / reliability
  dat2 <- dat
  for (i in seq_len(iter)) {
    # Simulate y
    dat2[["y"]] <- yhat + rnorm(n, sd = sigma)
    for (j in 1:2) {    
      dat2[[paste0("x", j)]] <- dat[[paste0("x", j)]] + rnorm(n, sd = meas_sd[j])  
    }
    # Estimate OLS
    results <- tidy(lm(y ~ x1 + x2, data = dat2))
  }
  summarize_params(results)
}

sims_measurement_error <-
  expand.grid(reliability1 = c(.1, .7, 1),
              reliability2 = c(.1, .7, 1),
              rho = c(0, 0.3, .7),
              obs = c(30, 100, 1000)) %>%
  group_by(rho, obs, reliability1, reliability2) %>%
  do({
    simulate_measurement_error(1500,
                          rho = .$rho,
                          n = .$obs,
                          reliability = c(.$reliability1, 
                                          .$reliability2))
  })
```

Create plots of the simulation results to answer the following questions:

- How is the bias of the coefficients related to the correlation between   x1 and x2, the reliability, and correlation between x's changes?
- How is the size of the sampling distribution of $\hat{beta}_1$ related to ... ?
- How is the bias and variance of the standard error of $\hat{\beta}_1$ ... related to?

## Omitted Variables

Simulate data from:
$$
Y = X_1 + \beta_2 X_2 + \varepsilon
$$
where $\Cor(X_1, X_2) = \rho$.
However, you estimate a regression with an omitted variable,
$$
y_i = \hat{\beta}_1 x_{1,i} + \hat\varepsilon
$$
You will run Monte Carlo simulations for various values of the correlation betwen $X_1$ and $X_2$, $\rho$, sample size, $n$, and parameter of $X_2$,  $\beta_2$.
You want to understand how this affects the sampling distribution of $\hat{\beta}_1$ (true value is $\beta_1 = 1$) and $\se{\hat{\beta}_1}$.

```{r}
simulate_ovb <- function(n, rho, beta2) {
  n <- 100
  rho <- 0
  r2 <- 0.5  
  beta <- c(0, 1, beta2)
  cormat <- matrix(c(1, rho, rho, 1), nrow = 2, ncol = 2)
  dat <- mvnormal_data(n, mu = c(0, 0), cor = cormat)
  yhat <- model.mat(~ x1 + x2, data = dat)
  sigma <- r2_to_sigma(yhat, r2)
  results <- vector(mode = "list", length = iter)
  for (i in seq_len(iter)) {
    # Simulate y
    dat[["y"]] <- yhat + rnorm(n, sd = sigma)
    # Estimate OLS
    results <- tidy(lm(y ~ x1, data = dat))
  }
  summarize_params(results)
}

sims_ovb <-
  expand.grid(rho = c(0, 0.3, .7),
              beta2 = c(1, 0.5, 0),
              obs = c(30, 100, 1000)) %>%
  group_by(rho, beta2, obs) %>%
  do({
    simulate_measurement_error(1500,
                          rho = .$rho,
                          n = .$obs,
                          reliability = c(.$reliability1, 
                                          .$reliability2))
  })
```

Create plots of the simulation results to answer the following questions:

- How is the bias of the coefficients related to the sample size ($n$) and correlation between $X_1$ and $X_2$ ($rho$), ?
- How is the size of the sampling distribution of the coefficients related to ... ?
- How is the bias of the standard error of the coefficients related to ...?


# More Nunn and Wantchekon (2011)

1. How do Nunn and Wantchekon handle omitted variable bias? Replicate their calculations for at least one regression?
2. How would measurement error problems in the measure of trust used by Wantchekon affect their estimate of the effect of slave exports on trust? In their measures of slave exports? In the control variables? 
3. Calculate the VIF factors for the regression in Table 1, Model 6. How much does the less-than-perfect collinearity of slave exports with other variables affect its standard error? 
